{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python notebook to run text extraction portion of a RAG. The same functionality is present in RAGdb.py if you just want to extract the text into a vector db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.auto import tqdm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118833cd36f74766b79ca922d482ddc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Text extraction from localRAGlib\n",
    "\n",
    "import fitz \n",
    "\n",
    "filepath = \"localRAGlib\"\n",
    "files = os.listdir(filepath)\n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Performs minor formatting on text.\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"- \", \"\").strip() #newlines and line breaks\n",
    "    text = text.replace(\" . .\", \"\")\n",
    "    # Other potential text formatting functions can go here\n",
    "\n",
    "    # Gelman text\n",
    "    text = text.replace(\"This book has been published by Cambridge University Press as Regression and Other Stories by Andrew Gelman, Jennifer Hill, and Aki Vehtari.This PDF is free to view and download for personal use only.Not for re-distribution, re-sale or use in derivative works.© Copyright by Andrew Gelman, Jennifer Hill, and Aki Vehtari 2020.The book web page https://avehtari.github.io/\", \"\")\n",
    "    text = text.replace(\"This electronic edition is for non-commercial purposes only.\", \"\")\n",
    "    text = text.replace(\"This book has been published by Cambridge University Press as Regression and Other Stories by Andrew Gelman, Jennifer Hill, and Aki Vehtari.This PDF is free to view and download for personal use only.Not for re-distribution, re-sale or use in derivative works.© Copyright by Andrew Gelman, Jennifer Hill, and Aki Vehtari 2020.The book web page https://avehtari.github.io/ROS-Examples/\", \"\")\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "pages_and_texts = []\n",
    "for file in tqdm(files):\n",
    "    pdf_path = os.path.join(filepath,file)\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_number, page in enumerate(doc):\n",
    "        text = page.get_text() \n",
    "        text = text_formatter(text)\n",
    "        # Filter out mostly empty pages and weirdly formatted pages\n",
    "        if len(text.split(\" \")) >= 50 and len(text) <= 5000: \n",
    "            pages_and_texts.append({\"document\": file,\n",
    "                                    \"page_number\": page_number, \n",
    "                                    \"page_char_count\": len(text),\n",
    "                                    \"page_word_count\": len(text.split(\" \")),\n",
    "                                    \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                    \"page_token_count\": len(text) / 4,  \n",
    "                                    \"text\": text})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8328\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8328.00</td>\n",
       "      <td>8328.00</td>\n",
       "      <td>8328.00</td>\n",
       "      <td>8328.00</td>\n",
       "      <td>8328.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>308.47</td>\n",
       "      <td>2267.38</td>\n",
       "      <td>390.80</td>\n",
       "      <td>20.42</td>\n",
       "      <td>566.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>200.02</td>\n",
       "      <td>800.21</td>\n",
       "      <td>135.96</td>\n",
       "      <td>16.97</td>\n",
       "      <td>200.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>243.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>60.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>140.00</td>\n",
       "      <td>1771.00</td>\n",
       "      <td>312.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>442.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>282.00</td>\n",
       "      <td>2216.00</td>\n",
       "      <td>386.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>554.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>459.00</td>\n",
       "      <td>2622.00</td>\n",
       "      <td>451.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>655.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>817.00</td>\n",
       "      <td>4988.00</td>\n",
       "      <td>2037.00</td>\n",
       "      <td>184.00</td>\n",
       "      <td>1247.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count      8328.00          8328.00          8328.00                  8328.00   \n",
       "mean        308.47          2267.38           390.80                    20.42   \n",
       "std         200.02           800.21           135.96                    16.97   \n",
       "min           0.00           243.00            50.00                     1.00   \n",
       "25%         140.00          1771.00           312.00                    13.00   \n",
       "50%         282.00          2216.00           386.00                    18.00   \n",
       "75%         459.00          2622.00           451.00                    23.00   \n",
       "max         817.00          4988.00          2037.00                   184.00   \n",
       "\n",
       "       page_token_count  \n",
       "count           8328.00  \n",
       "mean             566.84  \n",
       "std              200.05  \n",
       "min               60.75  \n",
       "25%              442.75  \n",
       "50%              554.00  \n",
       "75%              655.50  \n",
       "max             1247.00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(len(pages_and_texts)) #5374 pages total\n",
    "\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[This is a sentence., This is another sentence.]\n"
     ]
    }
   ],
   "source": [
    "# Chunk page data into smaller bits\n",
    "\n",
    "from spacy.lang.en import English \n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "#quick test\n",
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "assert len(list(doc.sents)) == 2\n",
    "print(list(doc.sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "13\n",
      "8\n",
      "125\n",
      "117\n",
      "95\n",
      "91\n",
      "84\n",
      "103\n",
      "94\n",
      "91\n",
      "1\n",
      "1\n",
      "69\n",
      "67\n",
      "71\n",
      "65\n",
      "77\n",
      "65\n",
      "71\n",
      "83\n",
      "73\n",
      "67\n",
      "74\n",
      "39\n",
      "35\n",
      "31\n",
      "42\n",
      "11\n",
      "37\n",
      "33\n",
      "78\n",
      "92\n",
      "81\n",
      "91\n",
      "91\n",
      "89\n",
      "70\n",
      "85\n",
      "88\n",
      "88\n",
      "87\n",
      "93\n",
      "92\n",
      "89\n",
      "88\n",
      "92\n",
      "89\n",
      "97\n",
      "45\n",
      "53\n",
      "46\n",
      "36\n",
      "40\n",
      "33\n",
      "38\n",
      "42\n",
      "35\n",
      "32\n",
      "34\n",
      "39\n",
      "39\n",
      "35\n",
      "33\n",
      "31\n",
      "47\n",
      "39\n",
      "37\n",
      "41\n",
      "40\n",
      "34\n",
      "32\n",
      "41\n",
      "31\n",
      "27\n",
      "44\n",
      "40\n",
      "33\n",
      "37\n",
      "25\n",
      "38\n",
      "39\n",
      "67\n",
      "67\n",
      "78\n",
      "80\n",
      "63\n",
      "63\n",
      "77\n",
      "75\n",
      "81\n",
      "74\n",
      "70\n",
      "82\n",
      "79\n",
      "81\n",
      "85\n",
      "74\n",
      "74\n",
      "69\n",
      "73\n",
      "81\n",
      "77\n",
      "66\n",
      "78\n",
      "80\n",
      "86\n",
      "77\n",
      "80\n",
      "81\n",
      "76\n",
      "85\n",
      "77\n",
      "78\n",
      "77\n",
      "79\n",
      "87\n",
      "81\n",
      "89\n",
      "84\n",
      "80\n",
      "87\n",
      "83\n",
      "71\n",
      "82\n",
      "82\n",
      "74\n",
      "75\n",
      "85\n",
      "83\n",
      "84\n",
      "76\n",
      "83\n",
      "75\n",
      "76\n",
      "80\n",
      "15\n",
      "20\n",
      "18\n",
      "4\n",
      "18\n",
      "16\n",
      "5\n",
      "13\n",
      "24\n",
      "8\n",
      "52\n",
      "92\n",
      "83\n",
      "92\n",
      "91\n",
      "93\n",
      "84\n",
      "80\n",
      "90\n",
      "75\n",
      "91\n",
      "88\n",
      "94\n",
      "92\n",
      "83\n",
      "86\n",
      "82\n",
      "58\n",
      "Total: 164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\npage = weird_pages[4]\\ntext = page[\"text\"]\\ndoc = nlp(text)\\nprint(list(doc.sents))\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploratory analysis on pages with large numbers of sentences\n",
    "# turns out most are notes or references\n",
    "\n",
    "weird_pages = []\n",
    "\n",
    "for page in pages_and_texts:\n",
    "    if page[\"page_word_count\"] >= 800 or page['page_sentence_count_raw'] >= 100:\n",
    "        weird_pages.append(page)\n",
    "\n",
    "for page in weird_pages:\n",
    "    text = page[\"text\"]\n",
    "    doc = nlp(text)\n",
    "    print(len(list(doc.sents)))\n",
    "    #print(list(doc.sents))\n",
    "\n",
    "print(\"Total:\",len(weird_pages))\n",
    "\n",
    "\"\"\"\n",
    "page = weird_pages[4]\n",
    "text = page[\"text\"]\n",
    "doc = nlp(text)\n",
    "print(list(doc.sents))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3adc54d5c52442c383a88747e6311bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19642\n"
     ]
    }
   ],
   "source": [
    "# Define split size to turn groups of sentences into chunks\n",
    "num_sentence_chunk_size = 10 \n",
    "\n",
    "def split_list(input_list: list, \n",
    "               slice_size: int) -> list[list[str]]:\n",
    "\n",
    "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "def make_dict(text_chunk, page_items):\n",
    "    chunk_dict = {\n",
    "        \"sentence_chunk\" : text_chunk,\n",
    "        \"document\" : page_items[\"document\"],\n",
    "        \"page_number\": page_items[\"page_number\"],\n",
    "        \"chunk_char_count\" : len(text_chunk),\n",
    "        \"chunk_word_count\" : len([word for word in text_chunk.split(\" \")]),\n",
    "        \"chunk_token_count\" : len(text_chunk) / 4,# 1 token = ~4 characters\n",
    "    }\n",
    "    return chunk_dict\n",
    "\n",
    "chunks_and_texts = []\n",
    "for item in tqdm(pages_and_texts):\n",
    "    doc = nlp(item['text'])\n",
    "    sentence_chunks = list(doc.sents)\n",
    "    text_chunks = split_list(sentence_chunks,num_sentence_chunk_size)\n",
    "    for chunk in text_chunks:\n",
    "        sentences = [str(sentence) for sentence in chunk]\n",
    "        joined_sentence_chunk =  \"\".join(sentences).replace(\"  \", \" \").strip()\n",
    "        num_chars = len(joined_sentence_chunk)\n",
    "        #overly short/long chunks get ignored\n",
    "        if num_chars > 2000 and num_chars < 4001:\n",
    "            chunk1, chunk2 = joined_sentence_chunk[:num_chars // 2 + 50], joined_sentence_chunk[num_chars // 2 - 50:]\n",
    "            chunk_dict1 = make_dict(chunk1, item)\n",
    "            chunks_and_texts.append(chunk_dict1)\n",
    "            chunk_dict2 = make_dict(chunk1, item)\n",
    "            chunks_and_texts.append(chunk_dict2)\n",
    "        elif num_chars <= 2000 and num_chars > 100:\n",
    "            chunk_dict = make_dict(joined_sentence_chunk, item)\n",
    "            chunks_and_texts.append(chunk_dict)\n",
    "\n",
    "print(len(chunks_and_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19642.00</td>\n",
       "      <td>19642.00</td>\n",
       "      <td>19642.00</td>\n",
       "      <td>19642.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>319.56</td>\n",
       "      <td>953.00</td>\n",
       "      <td>158.08</td>\n",
       "      <td>238.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>203.85</td>\n",
       "      <td>446.13</td>\n",
       "      <td>77.93</td>\n",
       "      <td>111.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>101.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>25.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>146.00</td>\n",
       "      <td>587.00</td>\n",
       "      <td>97.00</td>\n",
       "      <td>146.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>294.50</td>\n",
       "      <td>969.00</td>\n",
       "      <td>160.00</td>\n",
       "      <td>242.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>477.00</td>\n",
       "      <td>1281.00</td>\n",
       "      <td>211.00</td>\n",
       "      <td>320.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>817.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>945.00</td>\n",
       "      <td>500.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  chunk_char_count  chunk_word_count  chunk_token_count\n",
       "count     19642.00          19642.00          19642.00           19642.00\n",
       "mean        319.56            953.00            158.08             238.25\n",
       "std         203.85            446.13             77.93             111.53\n",
       "min           0.00            101.00             10.00              25.25\n",
       "25%         146.00            587.00             97.00             146.75\n",
       "50%         294.50            969.00            160.00             242.25\n",
       "75%         477.00           1281.00            211.00             320.25\n",
       "max         817.00           2000.00            945.00             500.00"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(chunks_and_texts)\n",
    "df.describe().round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk token count: 26.75 | Text: 262–263, exercise 15 on p. 329, or exercise 11 on p. 488, which all use a technique called “the sign test.”\n",
      "Chunk token count: 28.5 | Text: So the SE for the sum of 100 draws is √ 100 × 1/2 = 5.The number of heads will be around 50, give or take 5 or so.\n",
      "Chunk token count: 28.5 | Text: To get at the size of the chance error, the best thing to do is to repeat the measurement several times.The spread\n",
      "Chunk token count: 25.5 | Text: It has kept people from selling who did not like the way his stock was acting and would have liqui23.9\n",
      "Chunk token count: 25.5 | Text: My people say the market is entitled to a reaction and that I’ll be able to buy it back cheaper.So 5.6\n",
      "Chunk token count: 491.75 | Text: 536 22.FINITE MIXTURE MODELS restrict µ1 < µ2 < · · ·< µH in the prior distribution so that the higher indexed components have higher means.However, there are some clear drawbacks to such an approach.Most interesting models in applications are multivariate, and in multivariate cases it is not at all clear in most cases what type of restriction is appropriate.Even in the case of a univariate Gaussian location-scale mixture, it may be that we need multiple components with similar means but diﬀerent variances to provide a good ﬁt to the data.If the means are close together, then label switching can occur even if we place a strict order restriction on the means and hence the restriction does not fully solve the label ambiguity problem.In addition, placing a restriction on the parameters can lead to mixing problems and bias in which the prior favors pushing apart of parameters, with this leading to substantial overestimation of the diﬀerences in the components in the presence of sizeable posterior uncertainty.22.4 Unspeciﬁed number of mixture components To allow for a variable number of mixture components H, one can potentially also choose a prior for H such as a truncated Poisson, with reversible jump MCMC (see Section 12.3) then used for posterior computation.Such an approach is computationally intensive, and in practice it is much more common to ﬁt the model for varying choices of H and then use a goodness-of-ﬁt criterion penalized for model complexity (see Chapter 7) to choose <k. One computational approach is to use the EM algorithm to obtain a maximum likelihood estimate or posterior mode for the parameters for each of a variety of choices of H and then reporting the results for the H having the best marginal posterior probability or estimated predictive performance.As discussed in Section 7.4, the marginal posterior probability can be sensitive to the prior distribution, and DIC is not well justiﬁed theoretically in mixture models.\n",
      "Chunk token count: 460.75 | Text: 8.3 Piecewise-polynomial networks 117 Applying Inequality (1.2), Appendix 1, shows that VCdim(F) < 2L [(L 1)Wlog2(Z + 1) + Wlog2 (4WLpk/ In2) + 1].The second statement in the theorem follows from the fact that L <k < W. D This theorem implies that if we approximate an activation function with a piecewise-polynomial function, the resulting network has bounded VC-dimension.Suppose the activation function / increases monotonically and takes values between 0 and 1.Then there is a piecewiseconstant function / with 0(1/A) pieces that approximates / within A everywhere. (Using higher order polynomials instead of constants also allows the approximating function to match derivatives of /.) Then a feed-forward network with W parameters, L layers, and computation units with activation function / has VC-dimension O(WL(L + log2(WL/A))).So even though Theorem 7.1 shows that, for certain sigmoidal activation functions, small networks of units with these activation functions have infinite VC-dimension, there are many sigmoid functions (including the function defined in Theorem 7.1) for which networks of units with activation functions that accurately approximate these sigmoid functions have small VC-dimension.The class of functions that can be approximated accurately using piecewise-polynomial functions is large, and includes, for example, functions of bounded variation.The bounds given by Theorems 8.7 and 8.8 are nearly optimal.For networks with a fixed number of layers and a piecewise-polynomial activation function with a fixed number of pieces of fixed degree, Theorem 6.3 shows that, in the case of binary inputs and at least three layers, the VC-dimension grows as Wlog2 W, and so Theorem 8.8 cannot be improved by more than a constant factor.Similarly, Theorem 6.4 shows this in the case of real inputs and at least two layers.\n",
      "Chunk token count: 473.75 | Text: 66 3.Linear Methods for Regression Here U and V are N × p and p × p orthogonal matrices, with the columns of U spanning the column space of X, and the columns of V spanning the row space.D is a p × p diagonal matrix, with diagonal entries d1 ≥d2 ≥ · · · ≥dp ≥0 called the singular values of X. If one or more values dj = 0, X is singular.Using the singular value decomposition we can write the least squares ﬁtted vector as Xˆβls = X(XT X)−1XT y = UUT y, (3.46) after some simpliﬁcation.Note that UT y are the coordinates of y with respect to the orthonormal basis U. Note also the similarity with (3.33); Q and U are generally diﬀerent orthogonal bases for the column space of X (Exercise 3.8).Now the ridge solutions are Xˆβridge = X(XT X + λI)−1XT y = U D(D2 + λI)−1D UT y = p X j=1 uj d2 j d2 j + λuT j y, (3.47) where the uj are the columns of U. Note that since λ ≥0, we have d2 j/(d2 j + λ) ≤1.Like linear regression, ridge regression computes the coordinates of y with respect to the orthonormal basis U. It then shrinks these coordinates by the factors d2 j/(d2 j + λ).This means that a greater amount of shrinkage is applied to the coordinates of basis vectors with smaller d2 j. What does a small value of d2 j mean?The SVD of the centered matrix X is another way of expressing the principal components of the variables in X. The sample covariance matrix is given by S = XT X/N, and from (3.45) we have XT X = VD2VT , (3.48) which is the eigen decomposition of XT X (and of S, up to a factor N).The eigenvectors vj (columns of V) are also called the principal components (or Karhunen–Loeve) directions of X. The ﬁrst principal component direction v1 has the property that z1 = Xv1 has the largest sample variance amongst all normalized linear combinations of the columns of X. This sample variance is easily seen to be Var(z1) = Var(Xv1) = d2 1 N , (3.49) and in fact z1 = Xv1 = u1d1.\n",
      "Chunk token count: 495.25 | Text: [192] SECURITY ANALYSIS class made a better showing than those of companies with funded debt.But from this rather obvious fact it does not follow that all preferred stocks with bonds preceding are unsound investments, any more than it can be said that all second-mortgage bonds are inferior in quality to all first-mortgage bonds.Such a principle would entail the rejection of all public-utility preferred stocks (since they invariably have bonds ahead of them) although these are better regarded as a group than are the “nonbonded” industrial preferreds.Furthermore, in the extreme test of 1932, a substantial percentage of the preferred issues which held up were preceded by funded debt.2 To condemn a powerfully entrenched security such as General Electric preferred in 1933 because it had an infinitesimal bond issue ahead of it, would have been the height of absurdity.This example should illustrate forcibly the inherent unwisdom of subjecting investment selection to hard and fast rules of a qualitative character.In our view, the presence of bonds senior to a preferred stock is a fact which the investor must take carefully into account, impelling him to greater caution than he might otherwise exercise; but if the company’s exhibit is sufficiently impressive the preferred stock may still be accorded an investment rating.Total-deductions Basis of Calculation Recommended.In calculating the earnings coverage for preferred stocks with bonds preceding, it is absolutely essential that the bond interest and preferred dividend be taken together.The almost universal practice of stating the earnings on the preferred stock separately (in dollars per share) is exactly similar to, and as fallacious as, the prior-deductions method of computing the margin above interest charges on a junior bond.If the preferred stock issue is much smaller than the funded debt, the earnings per share will indicate that the preferred dividend is earned more times than is the bond interest.\n",
      "Chunk token count: 475.0 | Text: The prior parameters were set to Mk = log(0.7), Sk = log(10), and τk0 = log(2).The hierarchical model provides an essential framework for expressing the two sources of variation (or uncertainty) and combining them in the analysis.Joint posterior distribution for the hierarchical model For Bayesian inference, we obtain the posterior distribution (up to a multiplicative constant) for all the parameters of interest, given the data and the prior information, by multiplying all the factors in the hierarchical model: the data distribution, p(y|ψ, E, t, σ), the population model, p(ψ|µ, τ), and the prior distribution, p(µ, τ|M, S, τ0), p(ψ, µ, τ 2, σ2|y, E, t, φ, M, S, τ2 0 , ν) ∝p(y|ψ, φ, E, t, σ2)p(ψ|µ, τ 2)p(µ, τ 2|M, S, τ 2 0 )p(σ2) ∝   J / j=1 K / k=1 2 / m=1 / t N(log yjkmt| log gm(θk, Ej, t), σ2 m)  σ−2 1 σ−2 2 × × , K / k=1 L / l=1 Ntrunc(ψkl|µl, τ 2 l ) . ,L / l=1 N(µl|Ml, S2 l )Inv-χ2(τ 2 l |νl, τ 2 0l) . , (19.6) where ψ is the set of vectors of individual-level parameters, µ and τ are the vectors of population means and standard deviations, σ is the pair of measurement variances, y is the vector of concentration measurements, E and t are the exposure concentrations and times, and M, S, τ, and ν are the hyperparameters.We use the notation Ntrunc for the normal distribution truncated at the speciﬁed number of standard deviations from the mean.The indexes j, k, l, m, and t refer to replication, person, parameter, type of measurement (blood or air), and time of measurement.To compute (19.6) as a function of the parameters, data, and experimental conditions, the functions gm must be computed numerically over the range of time corresponding to the experimental measurements.Computation Our goals are ﬁrst to ﬁt a pharmacokinetic model to experimental data, and then to use the model to perform inferences about quantities of interest, such as the population distribution\n"
     ]
    }
   ],
   "source": [
    "#See what the outliers look like\n",
    "\n",
    "min_token_length = 30\n",
    "max_token_length = 450\n",
    "for row in df[df[\"chunk_token_count\"] <= min_token_length].sample(5).iterrows():\n",
    "    print(f'Chunk token count: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')\n",
    "\n",
    "for row in df[df[\"chunk_token_count\"] >= max_token_length].sample(5).iterrows():\n",
    "    print(f'Chunk token count: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn table of page embeddings into vector db for retrieval\n",
    "# We use https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "device = \"mps:0\"\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = df[\"sentence_chunk\"]\n",
    "# 2min 43 secs on GPU \n",
    "text_chunk_embeddings = embedding_model.encode(text_chunks,\n",
    "                                               batch_size=64,\n",
    "                                               convert_to_tensor=True)\n",
    "\n",
    "text_chunk_embeddings = text_chunk_embeddings.to('cpu').numpy()\n",
    "embeddings_df = pd.DataFrame(text_chunk_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#To load in df\\ndf = pd.read_csv(data_df_save_path)\\nembeddings_df = pd.read_csv(embeddings_df_save_path)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_df_save_path = \"localRAG.csv\"\n",
    "df.to_csv(data_df_save_path, index=False)\n",
    "\n",
    "embeddings_df_save_path = \"localRAG_embs.csv\"\n",
    "embeddings_df.to_csv(embeddings_df_save_path, index=False)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#To load in df\n",
    "df = pd.read_csv(data_df_save_path)\n",
    "embeddings_df = pd.read_csv(embeddings_df_save_path)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
